[
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "Population Data BC ‚Äî Vancouver, BC\nSep 2025 ‚Äì Present\n\nLed migration from CSV-based ingestion to Apache Parquet, reducing researcher ingestion time by 60%.\nIntegrated Apache Arrow to improve in-memory analytics and reduce query latency by 40%.\nBuilt reproducible Python ETL pipelines with unit tests, Makefiles, logging, and documentation.\nMigrated legacy pipelines to a modern Airflow-orchestrated architecture using DuckDB."
  },
  {
    "objectID": "experience.html#data-engineer-intern",
    "href": "experience.html#data-engineer-intern",
    "title": "Experience",
    "section": "",
    "text": "Population Data BC ‚Äî Vancouver, BC\nSep 2025 ‚Äì Present\n\nLed migration from CSV-based ingestion to Apache Parquet, reducing researcher ingestion time by 60%.\nIntegrated Apache Arrow to improve in-memory analytics and reduce query latency by 40%.\nBuilt reproducible Python ETL pipelines with unit tests, Makefiles, logging, and documentation.\nMigrated legacy pipelines to a modern Airflow-orchestrated architecture using DuckDB."
  },
  {
    "objectID": "experience.html#data-engineer-intern-1",
    "href": "experience.html#data-engineer-intern-1",
    "title": "Experience",
    "section": "Data Engineer Intern",
    "text": "Data Engineer Intern\nSamsung Electronics ‚Äî Vancouver, BC\nSep 2024 ‚Äì Aug 2025\n\nDeployed CI/CD pipelines via GitHub Actions for real-time PII tagging across 10+ streams processing 500M+ records/day.\nOrchestrated cross-region data transfers for 200+ datasets with 99% availability.\nAutomated ETL workflows for 50+ Tableau dashboards using Airflow.\nBuilt dbt pipelines ingesting FastAPI-backed application data.\nPerformed large-scale data quality analysis on Redshift datasets (&gt;20M records)."
  },
  {
    "objectID": "experience.html#software-developer-intern",
    "href": "experience.html#software-developer-intern",
    "title": "Experience",
    "section": "Software Developer Intern",
    "text": "Software Developer Intern\nVancouver Coastal Health ‚Äî Vancouver, BC\nMay 2024 ‚Äì Aug 2024\n\nDeveloped an R package for ingestion-layer data validation using testthat.\nMigrated ingestion from flat files to SQLite-based storage.\nMaintained analytical pipelines supporting clinical informatics systems."
  },
  {
    "objectID": "hobbies.html",
    "href": "hobbies.html",
    "title": "Hobbies & Interests",
    "section": "",
    "text": "Outside of work and school, I enjoy:\n\nüèãÔ∏è‚Äç‚ôÇÔ∏è Strength training and fitness\nüìà Personal finance & long-term investing\nü§ñ Keeping up with AI research and data tooling\nüåç Exploring data-for-social-good projects\nüßë‚Äçüè´ Teaching and mentoring students"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Built a reusable R package using devtools and usethis\nSimplified KNN classification workflows\nIntegrated CI/CD with automated testing and Docker"
  },
  {
    "objectID": "projects.html#wine-classify-r-package",
    "href": "projects.html#wine-classify-r-package",
    "title": "Projects",
    "section": "",
    "text": "Built a reusable R package using devtools and usethis\nSimplified KNN classification workflows\nIntegrated CI/CD with automated testing and Docker"
  },
  {
    "objectID": "projects.html#job-search-portal",
    "href": "projects.html#job-search-portal",
    "title": "Projects",
    "section": "Job Search Portal",
    "text": "Job Search Portal\n\nBuilt a PHP + OracleDB application managing 1,000+ job applications\nDesigned database schemas normalized to BCNF\nImplemented backend services using HTTP APIs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm a Statistics undergraduate at UBC and a Data Engineer with experience designing and scaling data platforms across healthcare, government research, and large-scale enterprise systems.\nMy interests sit at the intersection of:\n\nData engineering & analytics infrastructure\nDistributed systems and cloud platforms\nReproducible research and data quality\nPrivacy-aware data pipelines\n\nCurrently, I work with Airflow, Spark, dbt, Parquet, and Arrow to build reliable, production-grade pipelines.\n\n\n\n\nüìç Vancouver, Canada\n\nüéì BSc in Statistics @ UBC\n\nüß† GPA: 4.0 / 4.33\n\nüë®‚Äçüè´ Undergraduate TA ‚Äî Statistics"
  },
  {
    "objectID": "index.html#hi-im-kashish",
    "href": "index.html#hi-im-kashish",
    "title": "About Me",
    "section": "",
    "text": "I‚Äôm a Statistics undergraduate at UBC and a Data Engineer with experience designing and scaling data platforms across healthcare, government research, and large-scale enterprise systems.\nMy interests sit at the intersection of:\n\nData engineering & analytics infrastructure\nDistributed systems and cloud platforms\nReproducible research and data quality\nPrivacy-aware data pipelines\n\nCurrently, I work with Airflow, Spark, dbt, Parquet, and Arrow to build reliable, production-grade pipelines.\n\n\n\n\nüìç Vancouver, Canada\n\nüéì BSc in Statistics @ UBC\n\nüß† GPA: 4.0 / 4.33\n\nüë®‚Äçüè´ Undergraduate TA ‚Äî Statistics"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "Python ‚Äî data pipelines, ETL, testing\nSQL ‚Äî analytics, warehousing\nR ‚Äî data validation, statistical workflows\nJava, Bash"
  },
  {
    "objectID": "skills.html#programming",
    "href": "skills.html#programming",
    "title": "Skills",
    "section": "",
    "text": "Python ‚Äî data pipelines, ETL, testing\nSQL ‚Äî analytics, warehousing\nR ‚Äî data validation, statistical workflows\nJava, Bash"
  },
  {
    "objectID": "skills.html#data-engineering",
    "href": "skills.html#data-engineering",
    "title": "Skills",
    "section": "Data Engineering",
    "text": "Data Engineering\n\nApache Airflow\nApache Spark\ndbt\nApache Parquet & Arrow\nDuckDB"
  },
  {
    "objectID": "skills.html#cloud-platforms",
    "href": "skills.html#cloud-platforms",
    "title": "Skills",
    "section": "Cloud & Platforms",
    "text": "Cloud & Platforms\n\nGoogle Cloud\n\nBigQuery\nCloud Storage\nPub/Sub\nCloud Functions\nCloud Run\nComposer\n\n\n\nAWS\n\nS3\nRedshift\nAthena\nIAM"
  },
  {
    "objectID": "skills.html#databases-analytics",
    "href": "skills.html#databases-analytics",
    "title": "Skills",
    "section": "Databases & Analytics",
    "text": "Databases & Analytics\n\nPostgreSQL\nMongoDB\nOracleDB\nSQLite\nTableau\nGrafana"
  },
  {
    "objectID": "skills.html#engineering-practices",
    "href": "skills.html#engineering-practices",
    "title": "Skills",
    "section": "Engineering Practices",
    "text": "Engineering Practices\n\nCI/CD (GitHub Actions)\nDocker\nUnit testing & data validation\nData quality checks\nReproducible pipelines"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Writing",
    "section": "",
    "text": "I occasionally write about:\n\nData engineering lessons from internships\nAirflow, dbt, Spark patterns\nData quality & reproducibility\nCareer notes for students breaking into data\n\n\n\nPosts\n\nWhy Parquet + Arrow Changed Our Ingestion Pipeline"
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Why Parquet + Arrow Changed Our Ingestion Pipeline",
    "section": "",
    "text": "During my internship at Population Data BC, we migrated ingestion pipelines from CSV-based workflows to Apache Parquet with Arrow-backed in-memory analytics.\n\nKey Takeaways\n\nColumnar formats dramatically reduce I/O\nArrow enables zero-copy data sharing\nResearcher productivity improved immediately\n\nMore technical deep-dive coming soon."
  }
]