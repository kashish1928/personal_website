[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Writing",
    "section": "",
    "text": "While I do not intend to follow a niche, I‚Äôd occasionally write about:\n\nData engineering lessons from internships\nFinance\nCareer notes for students\nBook reviews\nShow recommendations\n\nI‚Äôll try to keep stuff organized in groups!\n\n\n\n\nWhy Parquet + Arrow Changed Our Ingestion Pipeline"
  },
  {
    "objectID": "blog.html#wip",
    "href": "blog.html#wip",
    "title": "Writing",
    "section": "",
    "text": "While I do not intend to follow a niche, I‚Äôd occasionally write about:\n\nData engineering lessons from internships\nFinance\nCareer notes for students\nBook reviews\nShow recommendations\n\nI‚Äôll try to keep stuff organized in groups!\n\n\n\n\nWhy Parquet + Arrow Changed Our Ingestion Pipeline"
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Kashish Joshipura",
    "section": "",
    "text": "Data Engineer Intern Sept 2025 ‚Äì Present\nThe Mission: Optimizing the bridge between massive provincial health datasets and academic research.\nKey Contributions:\n\nPerformance Engineering: Led the transition from legacy CSV ingestion to a high-performance Apache Parquet and Apache Arrow stack. This architecture shift slashed researcher data ingestion times by 60% and reduced query latency by 40%.\nInfrastructure Modernization: Re-engineered core pipelines into a modern, container-ready architecture. I implemented Airflow orchestration paired with DuckDB to allow for lightning-fast, reproducible local processing of large-scale records.\nReliability: Established a ‚ÄúDevOps for Data‚Äù culture by building Python ETL pipelines reinforced with unit tests, automated logging, and comprehensive documentation.\n\nStack: Python Airflow DuckDB Arrow Parquet\n\n\n\nData Engineer Intern Sep 2024 ‚Äì Aug 2025\nThe Mission: Managing global-scale data infrastructure and privacy compliance.\nKey Contributions:\n\nHigh-Volume Orchestration: Managed the real-time processing of 500M+ records per day. I deployed automated CI/CD pipelines via GitHub Actions that handled dynamic PII (Personal Identifiable Information) tagging across 10+ data streams.\nData Reliability: Owned the orchestration of cross-region transfers for 200+ datasets, maintaining a strict 99% availability SLA for downstream analytics teams.\nAnalytics Engineering: Leveraged dbt (data build tool) to transform raw FastAPI-backed application data into actionable insights, automating the workflows for over 50 executive Tableau dashboards.\n\nStack: AWS Redshift dbt Airflow GitHub Actions Tableau\n\n\n\nSoftware Developer Intern May 2024 ‚Äì Aug 2024\nThe Mission: Enhancing data integrity for clinical informatics systems.\nKey Contributions:\n\nValidation Frameworks: Developed a custom R package using the testthat framework to enforce data quality at the ingestion layer. This prevented ‚Äúdirty data‚Äù from entering downstream clinical pipelines.\nModern Storage: Spearheaded the migration of historical clinical records from unstructured flat files to a centralized SQLite-based storage system, enabling faster retrieval and more complex analytical queries.\nPipeline Maintenance: Maintained and optimized critical analytical pipelines that support real-time clinical informatics, ensuring healthcare providers had access to validated data.\n\nStack: R testthat SQLite Clinical Informatics Git"
  },
  {
    "objectID": "experience.html#professional-experience",
    "href": "experience.html#professional-experience",
    "title": "Kashish Joshipura",
    "section": "",
    "text": "Data Engineer Intern Sept 2025 ‚Äì Present\nThe Mission: Optimizing the bridge between massive provincial health datasets and academic research.\nKey Contributions:\n\nPerformance Engineering: Led the transition from legacy CSV ingestion to a high-performance Apache Parquet and Apache Arrow stack. This architecture shift slashed researcher data ingestion times by 60% and reduced query latency by 40%.\nInfrastructure Modernization: Re-engineered core pipelines into a modern, container-ready architecture. I implemented Airflow orchestration paired with DuckDB to allow for lightning-fast, reproducible local processing of large-scale records.\nReliability: Established a ‚ÄúDevOps for Data‚Äù culture by building Python ETL pipelines reinforced with unit tests, automated logging, and comprehensive documentation.\n\nStack: Python Airflow DuckDB Arrow Parquet\n\n\n\nData Engineer Intern Sep 2024 ‚Äì Aug 2025\nThe Mission: Managing global-scale data infrastructure and privacy compliance.\nKey Contributions:\n\nHigh-Volume Orchestration: Managed the real-time processing of 500M+ records per day. I deployed automated CI/CD pipelines via GitHub Actions that handled dynamic PII (Personal Identifiable Information) tagging across 10+ data streams.\nData Reliability: Owned the orchestration of cross-region transfers for 200+ datasets, maintaining a strict 99% availability SLA for downstream analytics teams.\nAnalytics Engineering: Leveraged dbt (data build tool) to transform raw FastAPI-backed application data into actionable insights, automating the workflows for over 50 executive Tableau dashboards.\n\nStack: AWS Redshift dbt Airflow GitHub Actions Tableau\n\n\n\nSoftware Developer Intern May 2024 ‚Äì Aug 2024\nThe Mission: Enhancing data integrity for clinical informatics systems.\nKey Contributions:\n\nValidation Frameworks: Developed a custom R package using the testthat framework to enforce data quality at the ingestion layer. This prevented ‚Äúdirty data‚Äù from entering downstream clinical pipelines.\nModern Storage: Spearheaded the migration of historical clinical records from unstructured flat files to a centralized SQLite-based storage system, enabling faster retrieval and more complex analytical queries.\nPipeline Maintenance: Maintained and optimized critical analytical pipelines that support real-time clinical informatics, ensuring healthcare providers had access to validated data.\n\nStack: R testthat SQLite Clinical Informatics Git"
  },
  {
    "objectID": "hobbies.html",
    "href": "hobbies.html",
    "title": "Hobbies & Interests",
    "section": "",
    "text": "I love staying active and trying new things. My favorites are:\n- Swimming üèä‚Äç‚ôÇÔ∏è ‚Äî nothing like the calm of the water to reset your brain\n- Running üèÉ‚Äç‚ôÇÔ∏è ‚Äî Vancouver runs are the best therapy\n- A little bit of every sport I can get my hands on üèÄ‚öΩüéæ ‚Äî keeps things fun and unpredictable\n\n\n\nI‚Äôm a binge-watcher at heart. My current favorites:\nShows:\n- Stranger Things ‚Äî suspense, mystery, and 80s nostalgia all rolled into one\n- Modern Family ‚Äî hilarious, heartwarming, and perfectly relatable\nMovies:\n- F1: The movie ‚Äî fast cars, big drama, and real-life racing chaos\n- Bhool Bhulaiyaa ‚Äî twists, thrills, and a touch of spookiness\n- Crazy, Stupid, Love ‚Äî romance, humor, and awkwardly perfect moments\n\n\n\n\nSome personal picks I‚Äôd recommend:\n- And Then There Were None ‚Äî classic mystery that keeps you guessing\n- Byculla to Bangkok ‚Äî a gripping true story with incredible storytelling\n- Wealthy Barber Returns ‚Äî fun, practical personal finance advice\n- Harry Potter and the Prisoner of Azkaban ‚Äî magic, nostalgia, and timeless adventure\n\n\n\nI enjoy learning about investing, budgeting, and planning for the future. It‚Äôs surprisingly fun when you see ideas actually work in real life!\nYou‚Äôd often find me on investopedia or reddit browsing r/personalFinanceCanada\n\n\n\nI enjoy the mix of speed, strategy, and engineering, and every race has me hoping, laughing, and occasionally facepalming (life of a Ferrari supporter)."
  },
  {
    "objectID": "hobbies.html#what-i-do-outside-of-work-school",
    "href": "hobbies.html#what-i-do-outside-of-work-school",
    "title": "Hobbies & Interests",
    "section": "",
    "text": "I love staying active and trying new things. My favorites are:\n- Swimming üèä‚Äç‚ôÇÔ∏è ‚Äî nothing like the calm of the water to reset your brain\n- Running üèÉ‚Äç‚ôÇÔ∏è ‚Äî Vancouver runs are the best therapy\n- A little bit of every sport I can get my hands on üèÄ‚öΩüéæ ‚Äî keeps things fun and unpredictable\n\n\n\nI‚Äôm a binge-watcher at heart. My current favorites:\nShows:\n- Stranger Things ‚Äî suspense, mystery, and 80s nostalgia all rolled into one\n- Modern Family ‚Äî hilarious, heartwarming, and perfectly relatable\nMovies:\n- F1: The movie ‚Äî fast cars, big drama, and real-life racing chaos\n- Bhool Bhulaiyaa ‚Äî twists, thrills, and a touch of spookiness\n- Crazy, Stupid, Love ‚Äî romance, humor, and awkwardly perfect moments\n\n\n\n\nSome personal picks I‚Äôd recommend:\n- And Then There Were None ‚Äî classic mystery that keeps you guessing\n- Byculla to Bangkok ‚Äî a gripping true story with incredible storytelling\n- Wealthy Barber Returns ‚Äî fun, practical personal finance advice\n- Harry Potter and the Prisoner of Azkaban ‚Äî magic, nostalgia, and timeless adventure\n\n\n\nI enjoy learning about investing, budgeting, and planning for the future. It‚Äôs surprisingly fun when you see ideas actually work in real life!\nYou‚Äôd often find me on investopedia or reddit browsing r/personalFinanceCanada\n\n\n\nI enjoy the mix of speed, strategy, and engineering, and every race has me hoping, laughing, and occasionally facepalming (life of a Ferrari supporter)."
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Why Parquet + Arrow Changed Our Ingestion Pipeline",
    "section": "",
    "text": "During my internship at Population Data BC, we migrated ingestion pipelines from CSV-based workflows to Apache Parquet with Arrow-backed in-memory analytics.\n\nKey Takeaways\n\nColumnar formats dramatically reduce I/O\nArrow enables zero-copy data sharing\nResearcher productivity improved immediately\n\nMore technical deep-dive coming soon."
  },
  {
    "objectID": "projects.html#wine-classify-r-package",
    "href": "projects.html#wine-classify-r-package",
    "title": "Projects",
    "section": "Wine Classify (R Package)",
    "text": "Wine Classify (R Package)\n\nBuilt a reusable R package using devtools and usethis\nSimplified KNN classification workflows\nIntegrated CI/CD with automated testing and Docker"
  },
  {
    "objectID": "projects.html#job-search-portal",
    "href": "projects.html#job-search-portal",
    "title": "Projects",
    "section": "Job Search Portal",
    "text": "Job Search Portal\n\nBuilt a PHP + OracleDB application managing 1,000+ job applications\nDesigned database schemas normalized to BCNF\nImplemented backend services using HTTP APIs"
  },
  {
    "objectID": "index.html#hi-im-kashish",
    "href": "index.html#hi-im-kashish",
    "title": "About Me",
    "section": "Hi, I‚Äôm Kashish üëã",
    "text": "Hi, I‚Äôm Kashish üëã\nI‚Äôm a Statistics student at UBC Vancouver who likes working with data - cleaning it, analyzing it, building things with it, and occasionally wondering why it‚Äôs broken in the first place.\nI‚Äôve spent time across data science, analytics, and data engineering, working on projects in healthcare, energy, and electronics sector. I enjoy figuring out how data moves from raw and messy to useful and understandable.\nRight now, I mostly work with Python and SQL, and spend a lot of time around tools like Airflow, Spark, dbt, Parquet, and Arrow. I like systems that are simple, reproducible, and don‚Äôt surprise you at 3am.\nWhen I‚Äôm not working with data, I‚Äôm either at the pool, catching up on sleep, or convincing myself I‚Äôll only watch one episode."
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "Python ‚Äî ETL pipelines, data analysis, testing\n\nSQL ‚Äî analytics, warehousing, querying big datasets\n\nR ‚Äî statistical workflows, data validation, reproducible analysis\nBash ‚Äî scripting & automation\n\n\n\n\n\nApache Airflow ‚Äî orchestrating workflows\n\nApache Spark ‚Äî distributed data processing\n\ndbt ‚Äî analytics engineering & transformations\n\nParquet & Arrow ‚Äî high-performance data storage & in-memory analytics\n\nDuckDB ‚Äî fast local analytics\n\n\n\n\n\n\n\nBigQuery, Cloud Storage, Pub/Sub\n\nCloud Functions, Cloud Run, Composer\n\n\n\n\n\nS3, Redshift, Athena, IAM\n\n\n\n\n\n\nPostgreSQL, MongoDB, DuckDB, SQLite\n\nTableau ‚Äî dashboards & visualization\n\nGrafana ‚Äî monitoring & metrics\n\n\n\n\n\nCI/CD ‚Äî GitHub Actions\n\nDocker ‚Äî containerized workflows\n\nUnit testing & data validation\n\nData quality checks & reproducible pipelines"
  },
  {
    "objectID": "skills.html#skills-expertise",
    "href": "skills.html#skills-expertise",
    "title": "Skills",
    "section": "",
    "text": "Python ‚Äî ETL pipelines, data analysis, testing\n\nSQL ‚Äî analytics, warehousing, querying big datasets\n\nR ‚Äî statistical workflows, data validation, reproducible analysis\nBash ‚Äî scripting & automation\n\n\n\n\n\nApache Airflow ‚Äî orchestrating workflows\n\nApache Spark ‚Äî distributed data processing\n\ndbt ‚Äî analytics engineering & transformations\n\nParquet & Arrow ‚Äî high-performance data storage & in-memory analytics\n\nDuckDB ‚Äî fast local analytics\n\n\n\n\n\n\n\nBigQuery, Cloud Storage, Pub/Sub\n\nCloud Functions, Cloud Run, Composer\n\n\n\n\n\nS3, Redshift, Athena, IAM\n\n\n\n\n\n\nPostgreSQL, MongoDB, DuckDB, SQLite\n\nTableau ‚Äî dashboards & visualization\n\nGrafana ‚Äî monitoring & metrics\n\n\n\n\n\nCI/CD ‚Äî GitHub Actions\n\nDocker ‚Äî containerized workflows\n\nUnit testing & data validation\n\nData quality checks & reproducible pipelines"
  }
]